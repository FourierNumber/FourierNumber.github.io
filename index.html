<!doctype html>
<html lang="en">
<head>
<title>FNE: Fourier Number Embedding</title>
<meta name="viewport" content="width=1500, user-scalable=no">
<meta name="description" content=">FNE: Fourier Number Embedding" />
<meta property="og:title" content="Fourier Number Embedding (FNE)">
<meta property="og:description" content="Fourier Number Embedding (FNE) directly maps numbers into their Fourier representations, improving efficiency and accuracy for large language models in arithmetic tasks.">
<meta property="og:url" content="https://kevinzhoutianyi.github.io/FNE_website/">
<meta property="og:image" content="/figs/add2modelacc_small (1).png"> <!-- Update with an actual image if needed -->
<meta name="twitter:card" content="summary" /> 
<link rel="icon" type="image/png" href="figs/allegro.png">
<link rel="icon" type="image/png" sizes="32x32" href="figs/allegro.png">
<link rel="icon" type="image/png" sizes="16x16" href="figs/allegro.png">
<link rel="icon" type="image/x-icon" href="figs/allegro.ico"> <!-- Optional .ico file -->

<script type="text/javascript" async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  let currentSlide = 1;
  const totalSlides = 8;  // Update this with the actual number of slides

  function changeSlide(n) {
      currentSlide += n;
      if (currentSlide < 1) {
          currentSlide = totalSlides;
      } else if (currentSlide > totalSlides) {
          currentSlide = 1;
      }
      document.getElementById("slide-img").src = `./figs/slides/slide${currentSlide}.png`;
  }
</script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
      const outlineList = document.getElementById("outline-list");
  
      // Manually add "Abstract" to the outline
      const abstractItem = document.createElement("li");
      const abstractLink = document.createElement("a");
      abstractLink.href = "#abstract";
      abstractLink.textContent = "Abstract";
  
      abstractItem.appendChild(abstractLink);
      outlineList.appendChild(abstractItem);
  
      // Dynamically add other sections with <h2> tags
      const headings = document.querySelectorAll(".container h2");
  
      headings.forEach((heading, index) => {
          const id = heading.id || `section-${index}`;
          heading.id = id;
  
          const listItem = document.createElement("li");
          const link = document.createElement("a");
          link.href = `#${id}`;
          link.textContent = heading.textContent;
  
          listItem.appendChild(link);
          outlineList.appendChild(listItem);
      });
  
      // Smooth scrolling effect
      document.querySelectorAll('.outline a').forEach(anchor => {
          anchor.addEventListener('click', function (e) {
              e.preventDefault();
              document.querySelector(this.getAttribute('href')).scrollIntoView({
                  behavior: 'smooth'
              });
          });
      });
  });
  </script>
  
  
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fourier Number Embedding (FNE)">
<meta name="twitter:description" content="Fourier Number Embedding (FNE) improves arithmetic performance in large language models by bypassing tokenization and using Fourier representations.">
<meta name="twitter:image" content="/figs/add2modelacc_small (1).png"> <!-- Update with an actual image if needed -->

<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link rel="stylesheet" href="style.css">
<script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
</head>
<body>

  <div class="outline">
    <ul id="outline-list"></ul>
</div>



<div class="container">
  
  <header class="text-center">
    <h1>FNE: Precise Single-Token Number Embeddings via Fourier Features</h1>

    <p><b>
      <a href="https://kevinzhoutianyi.github.io/">Tianyi Zhou</a>, 
      <a href="https://deqingfu.github.io/">Deqing Fu</a>, 
      <a href="https://scholar.google.com/citations?user=narJyMAAAAAJ&hl=en">Mahdi Soltanolkotabi</a>, 
      <a href="https://robinjia.github.io/">Robin Jia</a>, 
      <a href="https://vatsalsharan.github.io/">Vatsal Sharan</a>
    </b><br>
      <a href="mailto:tzhou029@usc.edu">tzhou029@usc.edu</a>, 
      <a href="mailto:deqingfu@usc.edu">deqingfu@usc.edu</a>, 
      <a href="mailto:soltanol@usc.edu">soltanol@usc.edu</a>, 
      <a href="mailto:robinjia@usc.edu">robinjia@usc.edu</a>, 
      <a href="mailto:vsharan@usc.edu">vsharan@usc.edu</a>
    </p>
    
        
    <div class="icon-container">
      <a href="https://github.com/KevinZhoutianyi/FNE" target="_blank">
        <img src="./figs/Github_logo_PNG8.png" alt="GitHub" class="icon github">
      </a>
      <a href="https://arxiv.org/abs/your-paper-id" target="_blank">
        <img src="./figs/ArXiv_logo_2022.svg" alt="Paper" class="icon arxiv">
      </a>
    </div>
  </header>
  

<section id="about">
  <section id="abstract">
    <p class="intro-text">
        Fourier Number Embedding (FNE) <br>
        directly maps numbers into their Fourier representations, bypassing the tokenization step entirely.<br>
        <strong>with Better Efficiency and Accuracy.</strong>
    </p>
</section>


<div class="highlights-grid">
  <div class="highlight-item">
    <h3 >Tokenization Comparison for <span class="highlight">12345.6789</span></h3>
    <table>
      <tr>
          <th>Tokenizer</th>
          <th>Tokenized Representation</th>
          <th>Tokens Used</th>
      </tr>
      <tr>
          <td>GPT-4, LLaMA3.2 (BPE)</td>
          <td><span style="color: #0066cc;">123</span> <span style="color: #009900;">45</span> <span style="color: #cc0000;">.</span> <span style="color: #ff9900;">678</span> <span style="color: #990099;">9</span></td>
          <td>5</td>
      </tr>
      <tr>
          <td>LLaMA2 (Digitwise tokenization)</td>
          <td><span style="color: #0066cc;">1</span> <span style="color: #009900;">2</span> <span style="color: #cc0000;">3</span> <span style="color: #ff9900;">4</span> <span style="color: #990099;">5</span> <span style="color: #990000;">.</span> <span style="color: #0066cc;">6</span> <span style="color: #009900;">7</span> <span style="color: #cc0000;">8</span> <span style="color: #ff9900;">9</span></td>
          <td>10</td>
      </tr>
      <tr>
          <td style="color: red; font-weight: bold;">FNE (Ours)</td>
          <td><span style="color: #0066cc;">12345.6789</span></td>
          <td style="color: red; font-weight: bold;">1</td>
      </tr>
    </table>
  </div>

  <div class="highlight-item">
    <h3>Accuracy Comparison</h3>
    <table>
        <tr>
            <th>Model Name</th>
            <th>Description</th>
            <th>Accuracy</th>
        </tr>
        <tr>
            <td>GPT-J</td>
            <td>Pretrained model, evaluated on integer addition for values up to <u>2</u>-digit numbers.</td>
            <td>80.5%</td>
        </tr>
        <tr>
            <td>Pythia-6.9B</td>
            <td>Pretrained model, evaluated on integer addition for values up to <u>2</u>-digit numbers.</td>
            <td>77.2%</td>
        </tr>
        <tr>
            <td>Llama3.1-8B</td>
            <td>Pretrained model, evaluated on integer addition for values up to <u>2</u>-digit numbers.</td>
            <td>98.0%</td>
        </tr>
        <tr style="color: red; font-weight: bold;">
            <td>Ours</td>
            <td>Trained from scratch, evaluated on integer addition up to <u>6</u>-digit numbers.</td>
            <td>100%</td>
        </tr>
        <tr style="color: red; font-weight: bold;">
            <td>Ours</td>
            <td>Trained from scratch, evaluated on integer addition  up to <u>50</u>-digit numbers.</td>
            <td>98.4%</td>
        </tr>
    </table>
</div>


  <div class="highlight-item">
    <h3>Training and Inference Efficiency</h3>
    <img src="figs/training_time_comparison.png" alt="Training Time Comparison" width="100%">
  </div>

  <div class="highlight-item">
    <h3>Data Efficiency</h3>
    <img src="figs/data_comparison.png" alt="Training Time Comparison" width="100%">
  </div>
</div>


</section>
  <section id="about">

    <h2>Methods</h2>
    <hr>
    <figure>
        <img src="./figs/animated_fig.gif" alt="Project animation" class="img-fluid">
      <figcaption>
        (a) Extract all numbers from the input. <br>
        (b) Use FNE to map each number to its embedding; the first two entries represent 18 mod 10, the next two 18 mod 100. <br>
        (c) Pad FNE with zeros, add it to word embeddings, and feed into the model. <br>
        (d) For each digit, take two entries from the last hidden state and find the closest number.
      </figcaption>
    </figure>
  </section>


  <section id="design" class="section">
    <h2>Why Design Like This?</h2>
    <hr>
    <p>As discussed in our pervious work [<a href="https://arxiv.org/pdf/2406.03445">Tianyi et al. (NeurIPS 2024)</a>], 
      LLMs naturally learn Fourier Features during pre-training. With these Fourier features, models are able to perform arithmetic with perfect accuracy. 
      However, due to the limitation of tokenization, LLMs can only embed numbers up to 520.
      <br>
      Below, we provide a simplified illustration of how pre-trained LLMs embed numbers and how this leads to Fourier Number Embedding (FNE).
    </p>
    <!-- <figure>
      <img src="./figs/animated_fig2.gif" alt="Project animation" class="img-fluid"> -->
    <!-- <figcaption>
      (a) Extract all numbers from the input. <br>
      (b) Use FNE to map each number to its embedding; the first two entries represent 18 mod 10, the next two 18 mod 100. <br>
      (c) Pad FNE with zeros, add it to word embeddings, and feed into the model. <br>
      (d) For each digit, take two entries from the last hidden state and find the closest number. -->
    <!-- </figcaption> -->
  <!-- </figure> -->
  <div class="slideshow-container">
    <img id="slide-img" src="./figs/slides/slide1.png" alt="Slide 1" class="img-fluid">
    <button class="prev" onclick="changeSlide(-1)">&#10094;</button>
    <button class="next" onclick="changeSlide(1)">&#10095;</button>
</div>
    <!-- <p> However, due to the limitation of tokenization, LLMs can only embed numbers up to 520.
    </p>
    <ul>
      <li>Numerical representations can be efficiently encoded without explicit tokenization</li>
      <li>Pre-trained models leverage these features for improved arithmetic performance</li>
      <li>Generalization across different numerical values emerges naturally</li>
    </ul> -->

    <p>Each column represents a wave function, using either cosine or sine with different periods.</p>
    <p>For a given number \( x \), some entries in its embedding encode \( \cos\left(\frac{2\pi}{10} x\right) \), while others encode \( \sin\left(\frac{2\pi}{10} x\right) \).</p>
    <p>Given \( \cos\left(\frac{2\pi}{10} x\right) \) and \( \sin\left(\frac{2\pi}{10} x\right) \), by using \( \arctan \), we can determine \( x \bmod 10 \), effectively extracting the number's unit digit.</p>
    <p>As shown in our previous work [<a href="https://arxiv.org/pdf/2406.03445">Tianyi et al. (NeurIPS 2024)</a>], LLMs learn to use \( x \bmod 2,5,10 \) to precisely represent numbers during pretraining.</p>
    <p>However, given the results \( x \bmod 2,5,10 \), we cannot recover \( x \) when \( x \geq 10 \). For example, \( 1 \) and \( 11 \) will give the same results.</p>
    <p><strong>Hence, we add features such as \( x \bmod 100,1000,\dots \) to ensure the embedding captures the precise representation of numbers.</strong></p>
    <p>In practice, each column is a superposition of waves with different periods.</p>
    <br><br>
    <h2>Example</h2>
    <hr>
    <p>Consider \( x = 41.7 \). Its Fourier Number Embedding is given by</p>
    \[
    [\phi(41.7, 1), \phi(41.7, 10), \phi(41.7, 100)],
    \]
    <p>where \( \phi(x, T) \) is defined as:</p>
    \[
    \phi(x, T) = [\cos(2\pi x / T), \sin(2\pi x / T)]
    \]
    <p>From these components, by using \( \arctan \), we can recover</p>
    \[
    [41.7 \bmod 1, 41.7 \bmod 10, 41.7 \bmod 100] =   [0.7, 1.7, 41.7],
    \]
    <br>
    <!-- <h3>Decoding Process</h3>
    <p>Given the last hidden state \([h_0,h_1,h_2,h_3 , ..]\), we have:</p>
    <h4>Loss Computation for the \(i\)-th digit</h4>
    <p>Compare the similarity between two entries in the hidden state and the Fourier Embedding of the \(i\)-th digit in the label: </p>
    \[
    L_{\mathrm{FNE}} (h, y, i) :=    L_{\mathrm{CE}}\!\Bigl(y_i,  (\underbrace{[
                    h[2i], ~h[2i+1]
                ]}_{1 \times 2}
                \cdot
                \underbrace{\begin{bmatrix}
                    \phi(0,10)\\
                    \vdots\\
                    \phi(9,10)
                \end{bmatrix}}_{2\times 10})
           \Bigr)
    \]
    
    <h4>Prediction for the \(i\)-th digit</h4>
    <p>Find the number whose Fourier embedding is closest to the two given entries in the hidden state.</p>
    
    \[
     \hat{y}_i := 
        \arg\max_{j \in \{0,\dots,9\}}
        \Bigl(
            \big[   h[2i], ~h[2i+1]
                \big]
                \cdot
                \big[\phi(j,10)\big]
        \Bigr).
    \]
     -->
    
      <section id="design" class="section">
        <h2>FAQ</h2>
        <hr>
        <p><strong><em>- Why can't we just simply use \( \bmod 100 \)? This contains all the information about 41.7.</em></strong></p>
        <p><em>If we used only \( T = 100 \), then the value of \( \phi(41.7, 100) \) would be nearly indistinguishable from \( \phi(41.8, 100) \), 
          causing the embedding to lose fine-grained information about less significant digits. However, with these chosen periods \( T \), we can capture all the digits.
          Note that each digit only requires two dimensions in the embedding. Therefore, we can encode numbers with up to (embedding dimension / 2) total digits, including both integer and decimal parts.</em></p>
        <br>
        <p><strong><em>- Will FNE affect the semantic meaning of numbers like years?</em></strong></p>
        <p>As discussed by [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/17359.pdf">Meng et al. (NeurIPS 2022)</a>], the semantic meaning or memory of tokens is often inferred from the MLP layers in transformer models. Since LLMs are typically equipped with sufficient capacity, the precise numerical embedding of numbers takes precedence over encoding their semantic meanings directly within the embeddings. Moreover, as noted by [<a href="https://arxiv.org/pdf/2405.17969">Yao et al. (arXiv 2024)</a>], LLMs are capable of developing specialized circuits to handle different query types. Consequently, FNE is designed to provide accurate numerical representations while allowing the model's architecture to manage semantic contexts independently.</p>  
        <br>
        </section>
    </section>
  </section>


  <section id="results" class="section">
    <h2>Empirical Results</h2>
    <hr>
    <p>We train Llama-3.2-1B from scratch with different number embedding methods and evaluate its performance on various arithmetic tasks. 
      Our <strong>Fourier Number Embedding (FNE)</strong> method demonstrates significant improvements in both <strong>data efficiency</strong> and <strong>parameter efficiency</strong>, 
      achieving <strong>99% accuracy with 64× less data</strong> compared to traditional embeddings. It also outperforms fine-tuned Llama-3.2 models and achieves perfect accuracy.</p>
      
    <p class="fig-caption">
      <strong>Figure:</strong> Comparison of accuracy trends for various arithmetic tasks with respect to model size and data size.
    </p>
  
    <!-- Enlarged Decimal Addition Figures -->
    <div class="figure-container full-width">
      <figure>
        <img src="./figs/adddecimaldataacc (1).png" alt="Decimal Addition Data Accuracy" class="figure-img">
        <figcaption>6-digit Decimal Addition: Model & Data size vs. Accuracy</figcaption>
      </figure>
      <figure>
        <img src="./figs/adddecimalmodelacc (1).png" alt="Decimal Addition Model Accuracy" class="figure-img">
        <figcaption>6-digit Decimal Addition: Model & Data size vs. Accuracy</figcaption>
      </figure>
    </div>
  
    <!-- Integer Addition & Subtraction (Row 1) -->
    <div class="figure-container">
      <figure>
        <img src="./figs/add2dataacc_small (1).png" alt="6-digit Integer Addition Data Accuracy" class="figure-img">
        <figcaption>6-digit Integer Addition: Model & Data size vs. Accuracy</figcaption>
      </figure>
      <figure>
        <img src="./figs/add2modelacc_small (1).png" alt="6-digit Integer Addition Model Accuracy" class="figure-img">
        <figcaption>6-digit Integer Addition: Model & Data size vs. Accuracy</figcaption>
      </figure>
      <figure>
        <img src="./figs/sub1dataacc_small (1).png" alt="5-digit Integer Subtraction Data Accuracy" class="figure-img">
        <figcaption>5-digit Integer Subtraction: Model & Data size vs. Accuracy</figcaption>
      </figure>
      <figure>
        <img src="./figs/sub1modelacc_small (1).png" alt="5-digit Integer Subtraction Model Accuracy" class="figure-img">
        <figcaption>5-digit Integer Subtraction: Model & Data size vs. Accuracy</figcaption>
      </figure>
    </div>
  
    <!-- Multiplication 1 & 2 (Row 2) -->
    <div class="figure-container">
      <figure>
        <img src="./figs/mul1dataacc_small (1).png" alt="3-digit Integer Multiplication Data Accuracy" class="figure-img">
        <figcaption>3-digit Integer Multiplication: Model & Data size vs. Accuracy</figcaption>
      </figure>
      <figure>
        <img src="./figs/mul1modelacc_small (1).png" alt="3-digit Integer Multiplication Model Accuracy" class="figure-img">
        <figcaption>3-digit Integer Multiplication: Model & Data size vs. Accuracy</figcaption>
      </figure>
      <figure>
        <img src="./figs/mul2dataacc_small (1).png" alt="4-digit Integer Multiplication Data Accuracy" class="figure-img">
        <figcaption>4-digit Integer Multiplication: Model & Data size vs. Accuracy</figcaption>
      </figure>
      <figure>
        <img src="./figs/mul2modelacc_small (1).png" alt="4-digit Integer Multiplication Model Accuracy" class="figure-img">
        <figcaption>4-digit Integer Multiplication: Model & Data size vs. Accuracy</figcaption>
      </figure>
    </div>
  
  
  </section>
  


<!-- 

  <section id="resources" class="section">
    <h2>Resources</h2>
    <p>Include links to important resources:</p>
    <ul>
      <li><a href="#">Project Documentation</a></li>
      <li><a href="#">Source Code (GitHub)</a></li>
      <li><a href="#">Demo or Colab Notebook</a></li>
    </ul>
  </section> -->

  <section id="how-to-cite" class="section">
    <h2>How to Cite</h2>
    <hr>
    <p>If you found this project useful, please cite our work as follows:</p>
    <pre>
      @article{zhou2024fne,
        title={FNE: Precise Single-Token Number Embeddings via Fourier Features},
        author={Tianyi Zhou, Deqing Fu, Mahdi Soltanolkotabi, Robin Jia, Vatsal Sharan},
        journal={arXiv preprint arXiv:???},
        year={2025},
        url={???}
      }
    </pre>
  </section>
  <section id="contact" class="section">
    <h2>Contact Me</h2>
    <hr>
    <p>If you would like to discuss applying <strong>Fourier Number Embedding (FNE)</strong> to quantization, data analysis, time series, or others—or explore adding new features to FNE, feel free to connect!</p>
    <p><strong>Email:</strong> <a href="mailto:tzhou029@usc.edu">tzhou029@usc.edu</a></p>
  </section>
  
<footer>
  <p>&copy; USC</p>
</footer>

</body>
</html>
